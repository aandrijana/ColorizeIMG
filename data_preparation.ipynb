{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8yGOszBFmaJ"
      },
      "source": [
        "# Data Preparation (important notes at the end)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu0ft9urF6yv"
      },
      "source": [
        "Number of L and AB images should match. Our dataset is separated into l (25000 grayscale images), ab1 (10000), ab2 (10000) and ab3 (10000). Since this project is done in Google Colab, to avoid crashes we will first use 10000 of grayscale images with only ab1. This may lead to smaller accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixAm751wloID"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37PHAPWvFxvd",
        "outputId": "bd517d0c-7367-4d3f-ed3d-306c190b7608"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gray image shape: (10000, 224, 224)\n",
            "AB image shape: (10000, 224, 224, 2)\n"
          ]
        }
      ],
      "source": [
        "l_channel = np.load(\"image_colorization_data/l/gray_scale.npy\")[:10000]\n",
        "ab = np.load(\"image_colorization_data/ab/ab/ab1.npy\")\n",
        "print(\"Gray image shape:\", l_channel.shape)\n",
        "print(\"AB image shape:\", ab.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0YQY3W4C0Xk"
      },
      "outputs": [],
      "source": [
        "def resize_l_ab(l_array, ab_array, target_shape=(128, 128)):\n",
        "    resized_l = []\n",
        "    resized_ab = []\n",
        "\n",
        "    for l_img, ab_img in zip(l_array, ab_array):\n",
        "        # Resizing L channel\n",
        "        l_resized = cv2.resize(l_img, target_shape, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "        # Resizin A and B channels separately\n",
        "        a_resized = cv2.resize(ab_img[:, :, 0], target_shape, interpolation=cv2.INTER_AREA) #cv2.INTER_AREA is an interpolation method used in OpenCV for resizing images.\n",
        "        b_resized = cv2.resize(ab_img[:, :, 1], target_shape, interpolation=cv2.INTER_AREA) #It uses pixel area relation for resampling, making it suitable for shrinking images (downsampling).\n",
        "        ab_resized = np.stack((a_resized, b_resized), axis=-1)\n",
        "\n",
        "        resized_l.append(l_resized)\n",
        "        resized_ab.append(ab_resized)\n",
        "\n",
        "    return np.array(resized_l), np.array(resized_ab)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l_channel, ab= resize_l_ab(l_channel, ab)\n",
        "print(\"Gray image shape:\", l_channel.shape)\n",
        "print(\"AB image shape:\", ab.shape) #printing new shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_ksJKw7VCNh",
        "outputId": "20337e76-af5f-4a27-8a1e-036d053a846a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gray image shape: (10000, 128, 128)\n",
            "AB image shape: (10000, 128, 128, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RL_7Ht4ivK5"
      },
      "source": [
        "Resized the images from 224x224 to 128x128 to reduce RAM usage and avoid crashes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOC29npZRp8T"
      },
      "source": [
        "### Filter Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZChNRMkwESS"
      },
      "outputs": [],
      "source": [
        "# Removing over/under-exposed images (L channel)\n",
        "mean_brightness = np.mean(l_channel, axis=(1, 2))\n",
        "# Tighten the brightness range based on the distribution\n",
        "valid_indices = np.where((mean_brightness >= 50) & (mean_brightness <= 170))[0]\n",
        "l_filtered = l_channel[valid_indices]\n",
        "ab_filtered = ab[valid_indices]\n",
        "\n",
        "# Removing low-colorfulness images (AB channels)\n",
        "colorfulness = np.std(ab, axis=(1, 2, 3))\n",
        "# Increased threshold to remove bland/grayscale images\n",
        "valid_indices = np.where(colorfulness > 10)[0]\n",
        "l_filtered = l_channel[valid_indices]\n",
        "ab_filtered = ab[valid_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqxaFrXxS0GZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "l_train, l_test, ab_train, ab_test = train_test_split(l_filtered, ab_filtered, test_size=0.1, random_state=42)\n",
        "l_train, l_val, ab_train, ab_val = train_test_split(l_train, ab_train, test_size=0.1, random_state=42)\n",
        "#since we are no dealing with classes, we just used the regular 42 seed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dimension is 3\n",
        "if l_filtered.ndim == 3:\n",
        "    l_filtered = l_filtered[..., np.newaxis]\n",
        "\n",
        "# train_test_split\n",
        "l_train, l_test, ab_train, ab_test = train_test_split(l_filtered, ab_filtered, test_size=0.1, random_state=42)\n",
        "l_train, l_val, ab_train, ab_val = train_test_split(l_train, ab_train, test_size=0.1, random_state=42)\n",
        "\n",
        "print(f\"Shape of l_train after fix: {l_train.shape}\") # Should be (..., 128, 128, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsnDBLwAiFhx",
        "outputId": "e638633d-c6ba-4e56-e098-9f6157b5c590"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of l_train after fix: (4302, 128, 128, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "L_IN_MIN, L_IN_MAX = 0.0, 255.0\n",
        "A_IN_MIN, A_IN_MAX = 43.0, 206.0\n",
        "B_IN_MIN, B_IN_MAX = 22.0, 222.0 #l, a, b channels"
      ],
      "metadata": {
        "id": "iGMm7bK5gVY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7qMjQh1SN3K"
      },
      "outputs": [],
      "source": [
        "def normalize_data(l_channel, ab_channels):\n",
        "    \"\"\"\n",
        "    Casts data to float32 and normalizes from the CUSTOM source ranges to [-1, 1].\n",
        "    \"\"\"\n",
        "    # Cast to float32 first\n",
        "    #Neural networks perform calculations with floating-point numbers, so this step is essential.\n",
        "    l_channel = tf.cast(l_channel, tf.float32)\n",
        "    ab_channels = tf.cast(ab_channels, tf.float32)\n",
        "\n",
        "    # Separate A and B channels from the (h, w, 2) tensor\n",
        "    # We use slicing to keep the final dimension, which makes concatenation easy\n",
        "    a_channel = ab_channels[..., 0:1]\n",
        "    b_channel = ab_channels[..., 1:2]\n",
        "\n",
        "    #Generic formula for mapping [min, max] to [-1, 1] is: 2 * (x - min) / (max - min) - 1\n",
        "    l_norm = 2 * (l_channel - L_IN_MIN) / (L_IN_MAX - L_IN_MIN) - 1\n",
        "    a_norm= 2 * (a_channel - A_IN_MIN) / (A_IN_MAX - A_IN_MIN) - 1\n",
        "    b_norm= 2 * (b_channel - B_IN_MIN) / (B_IN_MAX - B_IN_MIN) - 1\n",
        "\n",
        "    # Re-combine the normalized A and B channels\n",
        "    ab_norm = tf.concat([a_norm, b_norm], axis=-1)\n",
        "\n",
        "    return l_norm, ab_norm\n",
        "\n",
        "l_train, ab_train =normalize_data(l_train, ab_train)\n",
        "l_test, ab_test= normalize_data(l_test, ab_test)\n",
        "l_val, ab_val=normalize_data(l_val, ab_val)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def augment(l_channel, ab_channels):\n",
        "    \"\"\"Applies identical random horizontal flip to both L and AB channels.\"\"\" #reason explained below\n",
        "    if tf.random.uniform(()) > 0.5:\n",
        "        l_channel = tf.image.flip_left_right(l_channel)\n",
        "        ab_channels = tf.image.flip_left_right(ab_channels)\n",
        "    return l_channel, ab_channels\n"
      ],
      "metadata": {
        "id": "WrRMVhZIuusQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notes about data preparation:"
      ],
      "metadata": {
        "id": "d79S8P5L5L0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After performing changes that were previously announced we also took these steps:\n",
        "\n",
        "* The filtered data was split into training, validation, and test sets.\n",
        "* All L and AB channel data was then normalized from its original custom range to [-1, 1].\n",
        "* Data Augmentation: A simple but effective data augmentation strategy was implemented by applying random horizontal flipping to the training dataset.This technique effectively doubles the variety of the training data without needing new images. It teaches the model that the color of an object is independent of its left-right orientation, making the model more robust and less prone to overfitting. While other augmentations like rotations, zooms, or color jitter could be used, they add complexity and potential artifacts (e.g., black padding from rotations). Horizontal flipping is a \"safe\" and computationally inexpensive augmentation that provides significant benefits for this task."
      ],
      "metadata": {
        "id": "EZIBRofm5R0v"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "-byLKmxWObM8"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}